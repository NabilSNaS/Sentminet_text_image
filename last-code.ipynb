{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3792882,"sourceType":"datasetVersion","datasetId":2262518}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:11:51.087882Z","iopub.execute_input":"2024-12-09T23:11:51.088237Z","iopub.status.idle":"2024-12-09T23:11:51.094802Z","shell.execute_reply.started":"2024-12-09T23:11:51.088209Z","shell.execute_reply":"2024-12-09T23:11:51.093710Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import nltk\n\n# Download necessary NLTK data\nnltk.download('wordnet')  # Required for WordNetLemmatizer\nnltk.download('omw-1.4')  # Required for wordnet support for Lemmatizer\nnltk.download('stopwords')  # Required for stopword removal\nnltk.download('punkt')  # Required for tokenization\n\nimport nltk\nimport os\nimport zipfile\n\n# Define the NLTK data path\nnltk_data_path = \"/kaggle/working/nltk_data\"\nos.makedirs(nltk_data_path, exist_ok=True)\n\n# Add the path to NLTK's search paths\nnltk.data.path.append(nltk_data_path)\n\n# Download required resources\nnltk.download('wordnet', download_dir=nltk_data_path)\nnltk.download('omw-1.4', download_dir=nltk_data_path)\nnltk.download('stopwords', download_dir=nltk_data_path)\nnltk.download('punkt', download_dir=nltk_data_path)\n\n# Extract .zip files in /corpora\nfor zip_file in ['wordnet.zip', 'omw-1.4.zip']:\n    with zipfile.ZipFile(f\"{nltk_data_path}/corpora/{zip_file}\", 'r') as zip_ref:\n        zip_ref.extractall(f\"{nltk_data_path}/corpora\")\n\n# Verify extracted content\nprint(\"Extracted corpora:\", os.listdir(f\"{nltk_data_path}/corpora\"))\n\n# Test wordnet\nfrom nltk.corpus import wordnet\nprint(wordnet.synsets('test'))  # Should print a list of synsets\n\n# Test stopwords\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english')[:10])  # Should print the first 10 English stopwords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:11:51.103069Z","iopub.execute_input":"2024-12-09T23:11:51.103969Z","iopub.status.idle":"2024-12-09T23:11:52.847729Z","shell.execute_reply.started":"2024-12-09T23:11:51.103908Z","shell.execute_reply":"2024-12-09T23:11:52.846722Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Unzipping corpora/omw-1.4.zip.\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /kaggle/working/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nExtracted corpora: ['stopwords.zip', 'wordnet.zip', 'stopwords', 'omw-1.4', 'wordnet', 'omw-1.4.zip']\n[Synset('trial.n.02'), Synset('test.n.02'), Synset('examination.n.02'), Synset('test.n.04'), Synset('test.n.05'), Synset('test.n.06'), Synset('test.v.01'), Synset('screen.v.01'), Synset('quiz.v.01'), Synset('test.v.04'), Synset('test.v.05'), Synset('test.v.06'), Synset('test.v.07')]\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"ipywidgets\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:11:52.849743Z","iopub.execute_input":"2024-12-09T23:11:52.850454Z","iopub.status.idle":"2024-12-09T23:11:52.855280Z","shell.execute_reply.started":"2024-12-09T23:11:52.850379Z","shell.execute_reply":"2024-12-09T23:11:52.854385Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Path to the label file\nlabel_file_path = '/kaggle/input/mvsasingle/MVSA_Single/labelResultAll.txt'\n\n# Load the label data into a DataFrame\nlabels_df = pd.read_csv(label_file_path, sep='\\t')\n\n# Split the text and image labels into separate columns\nlabels_df[['text_label', 'image_label']] = labels_df['text,image'].str.split(',', expand=True)\n\n# Count the distribution of classes for both text and image labels\ntext_distribution = labels_df['text_label'].value_counts()\nimage_distribution = labels_df['image_label'].value_counts()\n\n# Align the distributions to ensure they have the same order of classes\nclasses = ['positive', 'neutral', 'negative']\ntext_counts = [text_distribution.get(cls, 0) for cls in classes]\nimage_counts = [image_distribution.get(cls, 0) for cls in classes]\n\n# Calculate the total count\ntotal_count = labels_df.shape[0]\n\n# Plotting\nx = np.arange(len(classes))  # label locations\nwidth = 0.35  # width of the bars\n\nfig, ax = plt.subplots(figsize=(10, 6))\nbars1 = ax.bar(x - width/2, text_counts, width, label='Text')\nbars2 = ax.bar(x + width/2, image_counts, width, label='Image')\n\n# Add labels and title\nax.set_xlabel('Classes')\nax.set_ylabel('Frequency')\nax.set_title('Distribution of Positive, Negative, and Neutral Classes for Text and Image')\nax.set_xticks(x)\nax.set_xticklabels(classes)\nax.legend()\n\n# Adding counts on top of each bar\nfor bar in bars1:\n    yval = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom')\n\nfor bar in bars2:\n    yval = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom')\n\n# Displaying the total data count in the top-right corner\nfig.text(0.95, 0.95, f'Total Data Count: {total_count}', ha='right', va='top', fontsize=12, color='blue', fontweight='bold')\n\n# Display the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:11:52.856823Z","iopub.execute_input":"2024-12-09T23:11:52.857219Z","iopub.status.idle":"2024-12-09T23:11:53.171766Z","shell.execute_reply.started":"2024-12-09T23:11:52.857177Z","shell.execute_reply":"2024-12-09T23:11:53.170652Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"!pip install torchsummary\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:11:53.173459Z","iopub.execute_input":"2024-12-09T23:11:53.173742Z","iopub.status.idle":"2024-12-09T23:12:02.885231Z","shell.execute_reply.started":"2024-12-09T23:11:53.173715Z","shell.execute_reply":"2024-12-09T23:12:02.883835Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchsummary in /opt/conda/lib/python3.10/site-packages (1.5.1)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!pip install gradio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:12:02.886962Z","iopub.execute_input":"2024-12-09T23:12:02.887335Z","iopub.status.idle":"2024-12-09T23:12:13.064315Z","shell.execute_reply.started":"2024-12-09T23:12:02.887287Z","shell.execute_reply":"2024-12-09T23:12:13.063079Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gradio in /opt/conda/lib/python3.10/site-packages (5.8.0)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.4.0)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.115.6)\nRequirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio) (0.4.0)\nRequirement already satisfied: gradio-client==1.5.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.5.1)\nRequirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.27.0)\nRequirement already satisfied: huggingface-hub>=0.25.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.26.5)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.5)\nRequirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.10.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.2)\nRequirement already satisfied: pillow<12.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (9.5.0)\nRequirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.8.2)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.0.19)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.2)\nRequirement already satisfied: ruff>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.8.2)\nRequirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.1.6)\nRequirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.10.0)\nRequirement already satisfied: starlette<1.0,>=0.40.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.41.3)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.13.2)\nRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.3)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.12.2)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.30.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.5.1->gradio) (2024.6.1)\nRequirement already satisfied: websockets<15.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.5.1->gradio) (12.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.7.4)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (3.15.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->gradio) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.20.1)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (1.26.18)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Import necessary modules\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport nltk\nimport time\nfrom torchinfo import summary\n\n# Set WandB environment variable to avoid socket issues\nos.environ[\"WANDB_SOCKET_TIMEOUT\"] = \"300\"\n\n# Initialize WandB\nwandb.init(\n    project=\"multimodal_sentiment_analysis\",\n    name=f\"fusion_model_{int(time.time())}\",\n    config={\n        \"batch_size\": 32,\n        \"learning_rate\": 2e-5,\n        \"epochs\": 30,\n        \"num_classes\": 2,\n        \"dropout_rate\": 0.6,\n        \"optimizer\": \"Adam\",\n        \"scheduler\": \"CosineAnnealingLR\"\n    }\n)\n\n# Initialize the WandbLogger for PyTorch Lightning\nwandb_logger = WandbLogger()\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# NLTK setup\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\n# Text preprocessing function\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\d+', '', text)\n    tokens = word_tokenize(text)\n    tokens = [word for word in tokens if word not in stop_words]\n    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n    return \" \".join(lemmas)\n\n# Dataset class\nclass MultimodalDataset(Dataset):\n    def __init__(self, texts, images, labels, tokenizer, transform):\n        self.texts = texts\n        self.images = images\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        image_path = self.images[idx]\n        label = self.labels[idx]\n\n        try:\n            image = Image.open(image_path).convert('RGB')\n        except FileNotFoundError:\n            image = Image.new('RGB', (224, 224))\n\n        processed_text = preprocess_text(text)\n        encoded_text = self.tokenizer(processed_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        image = self.transform(image)\n\n        return {\n            'text': encoded_text['input_ids'].squeeze(),\n            'attention_mask': encoded_text['attention_mask'].squeeze(),\n            'image': image,\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# PyTorch Lightning Model\nclass MultimodalSentimentModelPL(pl.LightningModule):\n    def __init__(self, bert_model, resnet_model, num_classes):\n        super(MultimodalSentimentModelPL, self).__init__()\n        self.text_model = bert_model\n        self.image_model = resnet_model\n\n        # Normalize text and image features to the same size (e.g., 512)\n        self.text_fc = nn.Linear(768, 512)\n        self.image_fc = nn.Linear(2048, 512)\n\n        # Classifier layers\n        self.fc1 = nn.Linear(512 + 512, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        self.dropout = nn.Dropout(0.6)\n        self.criterion = nn.CrossEntropyLoss()\n\n        # Initialize lists to store predictions and labels\n        self.test_preds = []\n        self.test_labels = []\n        \n    def forward(self, input_ids, attention_mask, image):\n        # Extract text features (BERT)\n        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)[1]  # Shape: [batch_size, 768]\n\n        # Extract image features (ResNet)\n        image_output = self.image_model(image)  # Shape: [batch_size, 2048]\n\n        # Normalize feature dimensions to the same size (e.g., 512)\n        text_output = self.text_fc(text_output)  # Shape: [batch_size, 512]\n        image_output = self.image_fc(image_output)  # Shape: [batch_size, 512]\n\n        # Combine normalized features\n        combined = torch.cat((text_output, image_output), dim=1)  # Shape: [batch_size, 1024]\n        x = self.fc1(combined)\n        x = torch.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        input_ids = batch['text']\n        attention_mask = batch['attention_mask']\n        images = batch['image']\n        labels = batch['label']\n        outputs = self(input_ids, attention_mask, images)\n        loss = self.criterion(outputs, labels)\n        preds = torch.argmax(outputs, dim=1)\n        accuracy = (preds == labels).float().mean()\n\n        # Log metrics at epoch level only\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"train_accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        input_ids = batch['text']\n        attention_mask = batch['attention_mask']\n        images = batch['image']\n        labels = batch['label']\n        outputs = self(input_ids, attention_mask, images)\n        loss = self.criterion(outputs, labels)\n        preds = torch.argmax(outputs, dim=1)\n        accuracy = (preds == labels).float().mean()\n\n        # Log metrics at epoch level only\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        input_ids = batch['text']\n        attention_mask = batch['attention_mask']\n        images = batch['image']\n        labels = batch['label']\n        outputs = self(input_ids, attention_mask, images)\n        loss = self.criterion(outputs, labels)\n        preds = torch.argmax(outputs, dim=1)\n        accuracy = (preds == labels).float().mean()\n\n        # Append predictions and labels for evaluation\n        self.test_preds.append(preds.cpu().numpy())\n        self.test_labels.append(labels.cpu().numpy())\n\n        # Log test metrics\n        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n        self.log(\"test_accuracy\", accuracy, prog_bar=True, logger=True)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=2e-5, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n    # Function to load and filter labels\ndef load_labels(result_file):\n    labels = {}\n    with open(result_file, 'r') as file:\n        next(file)  # Skip header\n        for line in file:\n            parts = line.strip().split('\\t')\n            text_id = int(parts[0])\n            label_string = parts[1].strip()\n            text_label, image_label = label_string.split(',')\n            if text_label == image_label and text_label in ['positive', 'negative']:\n                labels[text_id] = 1 if text_label == \"positive\" else 0\n    print(f\"Loaded {len(labels)} valid labels.\")\n    return labels\n\n# Function to align text, image paths, and labels\ndef align_data(data_folder, labels):\n    aligned_texts, aligned_images, aligned_labels = [], [], []\n    for text_id, label in labels.items():\n        text_path = os.path.join(data_folder, f\"{text_id}.txt\")\n        image_path = os.path.join(data_folder, f\"{text_id}.jpg\")\n        if os.path.exists(text_path) and os.path.exists(image_path):\n            aligned_texts.append(text_path)\n            aligned_images.append(image_path)\n            aligned_labels.append(label)\n    print(f\"Aligned {len(aligned_texts)} items.\")\n    return aligned_texts, aligned_images, aligned_labels\n\n# Function to load text and images\ndef load_text_and_images(data_folder, result_file):\n    labels = load_labels(result_file)\n    return align_data(data_folder, labels)\n\n# Preprocessing\ndata_folder = \"/kaggle/input/mvsasingle/MVSA_Single/data\"\nresult_file = \"/kaggle/input/mvsasingle/MVSA_Single/labelResultAll.txt\"\n\ntexts, images, labels = load_text_and_images(data_folder, result_file)\n\nif len(texts) > 0:\n    train_texts, temp_texts, train_images, temp_images, train_labels, temp_labels = train_test_split(\n        texts, images, labels, test_size=0.3, random_state=42, stratify=labels\n    )\n    val_texts, test_texts, val_images, test_images, val_labels, test_labels = train_test_split(\n        temp_texts, temp_images, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n    )\n\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    train_transform = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n    val_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor()])\n\n    train_dataset = MultimodalDataset(train_texts, train_images, train_labels, tokenizer, train_transform)\n    val_dataset = MultimodalDataset(val_texts, val_images, val_labels, tokenizer, val_transform)\n    test_dataset = MultimodalDataset(test_texts, test_images, test_labels, tokenizer, val_transform)\n\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n    bert_model = BertModel.from_pretrained('bert-base-uncased')\n    resnet_model = models.resnet50(pretrained=True)\n    resnet_model.fc = nn.Identity()\n\n    model = MultimodalSentimentModelPL(bert_model, resnet_model, num_classes=2)\n\n    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=True, mode=\"min\")\n    checkpoint_callback = ModelCheckpoint(\n        monitor=\"val_loss\", dirpath=\"./checkpoints\", filename=\"best-checkpoint\", save_top_k=1, mode=\"min\"\n    )\n\n    trainer = pl.Trainer(\n        max_epochs=30,\n        callbacks=[early_stopping, checkpoint_callback],\n        logger=wandb_logger,\n        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\"\n    )\n    trainer.fit(model, train_loader, val_loader)\n    trainer.test(model, dataloaders=test_loader)\n\n    # Define the Gradio interface\n    import gradio as gr\n\n    def predict_sentiment(text, image):\n        model.eval()\n        model.to(device)\n        encoded_text = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        input_ids = encoded_text['input_ids'].to(device)\n        attention_mask = encoded_text['attention_mask'].to(device)\n        if image is not None:\n            image = Image.open(image).convert('RGB')\n            image = val_transform(image).unsqueeze(0).to(device)\n        else:\n            image = torch.zeros((1, 3, 224, 224)).to(device)\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, image=image)\n            probabilities = torch.softmax(outputs, dim=1).squeeze().cpu().numpy()\n        labels = ['Negative', 'Positive']\n        predicted_label = labels[np.argmax(probabilities)]\n        confidence = np.max(probabilities)\n        return f\"Prediction: {predicted_label} ({confidence:.2%} confidence)\"\n\n    interface = gr.Interface(\n        fn=predict_sentiment,\n        inputs=[\n            gr.Textbox(lines=2, placeholder=\"Enter text here...\", label=\"Input Text\"),\n            gr.Image(type=\"filepath\", label=\"Upload an Image\"),\n        ],\n        outputs=gr.Text(label=\"Prediction\"),\n        title=\"Multimodal Sentiment Analysis\",\n        description=\"Provide a text input and an image to predict sentiment (Positive/Negative).\"\n    )\n    interface.launch()\n\n    wandb.finish()\nelse:\n    print(\"No valid data points found after preprocessing.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:12:13.066593Z","iopub.execute_input":"2024-12-09T23:12:13.067005Z","iopub.status.idle":"2024-12-09T23:19:52.157445Z","shell.execute_reply.started":"2024-12-09T23:12:13.066961Z","shell.execute_reply":"2024-12-09T23:19:52.156483Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:cxeeva5e) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▆▇█</td></tr><tr><td>train_loss</td><td>█▇▅▄▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇██</td></tr><tr><td>val_accuracy</td><td>▁██▇▆▆▅</td></tr><tr><td>val_loss</td><td>▃▁▁▂▅██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>6</td></tr><tr><td>train_accuracy</td><td>0.88822</td></tr><tr><td>train_loss</td><td>0.28504</td></tr><tr><td>trainer/global_step</td><td>328</td></tr><tr><td>val_accuracy</td><td>0.69497</td></tr><tr><td>val_loss</td><td>0.70118</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fusion_model_1733784734</strong> at: <a href='https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis/runs/cxeeva5e' target=\"_blank\">https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis/runs/cxeeva5e</a><br/> View project at: <a href='https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis' target=\"_blank\">https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241209_225238-cxeeva5e/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:cxeeva5e). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.19.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241209_231213-8rra6j49</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis/runs/8rra6j49' target=\"_blank\">fusion_model_1733785933</a></strong> to <a href='https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis' target=\"_blank\">https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis/runs/8rra6j49' target=\"_blank\">https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis/runs/8rra6j49</a>"},"metadata":{}},{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nLoaded 2122 valid labels.\nAligned 2122 items.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints exists and is not empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (47) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50d8335e67994963b6319b93b9a5b680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e4086d466e74f1bad4547b53e3fd2f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.746081531047821    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.637404203414917    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.746081531047821     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.637404203414917     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://841c0dfc89740e5956.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://841c0dfc89740e5956.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.027 MB uploaded\\r'), FloatProgress(value=0.05105010343255847, max=1.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▂▄▅▇▇█</td></tr><tr><td>train_loss</td><td>█▇▅▄▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▃▃▄▄▆▆▇▇███</td></tr><tr><td>val_accuracy</td><td>▁█▇▆▄▅▅</td></tr><tr><td>val_loss</td><td>▃▁▁▃▅▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>test_accuracy</td><td>0.74608</td></tr><tr><td>test_loss</td><td>0.6374</td></tr><tr><td>train_accuracy</td><td>0.89293</td></tr><tr><td>train_loss</td><td>0.2765</td></tr><tr><td>trainer/global_step</td><td>329</td></tr><tr><td>val_accuracy</td><td>0.7044</td></tr><tr><td>val_loss</td><td>0.70682</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fusion_model_1733785933</strong> at: <a href='https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis/runs/8rra6j49' target=\"_blank\">https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis/runs/8rra6j49</a><br/> View project at: <a href='https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis' target=\"_blank\">https://wandb.ai/hafija-pstu-cse-university-of-rhode-island/multimodal_sentiment_analysis</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241209_231213-8rra6j49/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# import os\n# import numpy as np\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import torchvision.transforms as transforms\n# import torchvision.models as models\n# from torch.utils.data import Dataset, DataLoader\n# from transformers import BertTokenizer, BertModel\n# import pytorch_lightning as pl\n# from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n# from pytorch_lightning.loggers import WandbLogger\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n# from PIL import Image\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import wandb\n# from nltk.corpus import stopwords\n# from nltk.tokenize import word_tokenize\n# from nltk.stem import WordNetLemmatizer\n# import re\n# import nltk\n# import time\n# import matplotlib.pyplot as plt\n\n# # Set WandB environment variable to avoid socket issues\n# os.environ[\"WANDB_SOCKET_TIMEOUT\"] = \"300\"\n\n# # Initialize WandB with a unique run name and log hyperparameters\n# wandb.init(\n#     project=\"multimodal_sentiment_analysis\",\n#     name=f\"fusion_model_{int(time.time())}\",\n#     config={\n#         \"batch_size\": 32,\n#         \"learning_rate\": 2e-5,\n#         \"epochs\": 30,\n#         \"num_classes\": 2,\n#         \"dropout_rate\": 0.6,\n#         \"optimizer\": \"Adam\",\n#         \"scheduler\": \"CosineAnnealingLR\"\n#     }\n# )\n\n# # Initialize the WandbLogger for PyTorch Lightning\n# wandb_logger = WandbLogger()\n\n# # Device configuration\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # NLTK setup\n# nltk.download('stopwords')\n# nltk.download('punkt')\n# nltk.download('wordnet')\n\n# stop_words = set(stopwords.words(\"english\"))\n# lemmatizer = WordNetLemmatizer()\n\n# # Text preprocessing function\n# def preprocess_text(text):\n#     text = text.lower()\n#     text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n#     text = re.sub(r'[^\\w\\s]', '', text)\n#     text = re.sub(r'\\d+', '', text)\n#     tokens = word_tokenize(text)\n#     tokens = [word for word in tokens if word not in stop_words]\n#     lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n#     return \" \".join(lemmas)\n\n# # Dataset class\n# class MultimodalDataset(Dataset):\n#     def __init__(self, texts, images, labels, tokenizer, transform):\n#         self.texts = texts\n#         self.images = images\n#         self.labels = labels\n#         self.tokenizer = tokenizer\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.texts)\n\n#     def __getitem__(self, idx):\n#         text = self.texts[idx]\n#         image_path = self.images[idx]\n#         label = self.labels[idx]\n\n#         try:\n#             image = Image.open(image_path).convert('RGB')\n#         except FileNotFoundError:\n#             image = Image.new('RGB', (224, 224))\n\n#         processed_text = preprocess_text(text)\n#         encoded_text = self.tokenizer(processed_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n#         image = self.transform(image)\n\n#         return {\n#             'text': encoded_text['input_ids'].squeeze(),\n#             'attention_mask': encoded_text['attention_mask'].squeeze(),\n#             'image': image,\n#             'label': torch.tensor(label, dtype=torch.long)\n#         }\n\n# # PyTorch Lightning Model\n# class MultimodalSentimentModelPL(pl.LightningModule):\n#     def __init__(self, bert_model, resnet_model, num_classes):\n#         super(MultimodalSentimentModelPL, self).__init__()\n#         self.text_model = bert_model\n#         self.image_model = resnet_model\n#         self.fc1 = nn.Linear(768 + 2048, 512)\n#         self.fc2 = nn.Linear(512, num_classes)\n#         self.dropout = nn.Dropout(0.6)\n#         self.criterion = nn.CrossEntropyLoss()\n\n#         # Store values for plotting\n#         self.train_losses = []\n#         self.val_losses = []\n#         self.test_losses = []  # For test losses\n#         self.train_accuracies = []\n#         self.val_accuracies = []\n#         self.test_accuracies = []  # For test accuracies\n\n#     def forward(self, input_ids, attention_mask, image):\n#         input_ids = input_ids.to(self.device)\n#         attention_mask = attention_mask.to(self.device)\n#         image = image.to(self.device)\n        \n#         text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n#         image_output = self.image_model(image)\n#         combined = torch.cat((text_output, image_output), dim=1)\n#         x = self.fc1(combined)\n#         x = torch.relu(x)\n#         x = self.dropout(x)\n#         x = self.fc2(x)\n#         return x\n\n#     def training_step(self, batch, batch_idx):\n#         input_ids = batch['text']\n#         attention_mask = batch['attention_mask']\n#         images = batch['image']\n#         labels = batch['label']\n#         outputs = self(input_ids, attention_mask, images)\n#         loss = self.criterion(outputs, labels)\n#         preds = torch.argmax(outputs, dim=1)\n#         accuracy = (preds == labels).float().mean()\n\n#         # Log train loss and accuracy\n#         self.train_losses.append(loss.item())\n#         self.train_accuracies.append(accuracy.item())\n#         self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n#         self.log(\"train_accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n#         return loss\n\n#     def validation_step(self, batch, batch_idx):\n#         input_ids = batch['text']\n#         attention_mask = batch['attention_mask']\n#         images = batch['image']\n#         labels = batch['label']\n#         outputs = self(input_ids, attention_mask, images)\n#         loss = self.criterion(outputs, labels)\n#         preds = torch.argmax(outputs, dim=1)\n#         accuracy = (preds == labels).float().mean()\n\n#         # Log validation loss and accuracy\n#         self.val_losses.append(loss.item())\n#         self.val_accuracies.append(accuracy.item())\n#         self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n#         self.log(\"val_accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n#         return loss\n\n#     # Add the test_step method\n#     def test_step(self, batch, batch_idx):\n#         input_ids = batch['text']\n#         attention_mask = batch['attention_mask']\n#         images = batch['image']\n#         labels = batch['label']\n#         outputs = self(input_ids, attention_mask, images)\n#         loss = self.criterion(outputs, labels)\n#         preds = torch.argmax(outputs, dim=1)\n#         accuracy = (preds == labels).float().mean()\n\n#         # Log test loss and accuracy\n#         self.test_losses.append(loss.item())\n#         self.test_accuracies.append(accuracy.item())\n#         self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n#         self.log(\"test_accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n#         return loss\n\n#     def on_epoch_end(self):\n#         # Create the loss and accuracy plots\n#         fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n#         # Plot Train and Validation Loss\n#         axes[0].plot(range(len(self.train_losses)), self.train_losses, label='Train Loss', color='blue')\n#         axes[0].plot(range(len(self.val_losses)), self.val_losses, label='Validation Loss', color='orange')\n#         axes[0].set_xlabel('Epoch')\n#         axes[0].set_ylabel('Loss')\n#         axes[0].set_title('Train and Validation Loss')\n#         axes[0].legend()\n\n#         # Plot Train and Validation Accuracy\n#         axes[1].plot(range(len(self.train_accuracies)), self.train_accuracies, label='Train Accuracy', color='blue')\n#         axes[1].plot(range(len(self.val_accuracies)), self.val_accuracies, label='Validation Accuracy', color='orange')\n#         axes[1].set_xlabel('Epoch')\n#         axes[1].set_ylabel('Accuracy')\n#         axes[1].set_title('Train and Validation Accuracy')\n#         axes[1].legend()\n\n#         # Log the plot to WandB\n#         wandb.log({\n#             \"Train vs Validation Loss and Accuracy\": wandb.Image(fig)\n#         })\n\n#         # Close the plot to free memory\n#         plt.close(fig)\n\n#     def configure_optimizers(self):\n#         optimizer = optim.Adam(self.parameters(), lr=2e-5, weight_decay=1e-4)\n#         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n#         return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n\n\n# # Load labels from the result file\n# def load_labels(result_file):\n#     \"\"\"Load and filter labels, keeping only matched pairs (positive-positive, negative-negative) and using a single label.\"\"\"\n#     labels = {}\n#     with open(result_file, 'r') as file:\n#         next(file)  # Skip header if present\n#         for line in file:\n#             parts = line.strip().split('\\t')\n#             text_id = int(parts[0])\n#             label_string = parts[1].strip()\n\n#             # Split label string into text_label and image_label\n#             text_label, image_label = label_string.split(',')\n\n#             # Include only matched pairs (positive-positive or negative-negative)\n#             if text_label == image_label and text_label in ['positive', 'negative']:\n#                 # Map to a single label: 1 for Positive, 0 for Negative\n#                 labels[text_id] = 1 if text_label == \"positive\" else 0\n\n#     print(f\"Loaded {len(labels)} valid positive-positive or negative-negative labels.\")\n#     return labels\n\n# # Align text, images, and labels based on index\n# def align_data(data_folder, labels):\n#     \"\"\"Align text, image paths, and labels.\"\"\"\n#     aligned_texts, aligned_images, aligned_labels = [], [], []\n#     for text_id, label in labels.items():\n#         text_file = f\"{text_id}.txt\"\n#         image_file = f\"{text_id}.jpg\"\n\n#         if not os.path.exists(os.path.join(data_folder, text_file)):\n#             text_file = f\"{text_id}.txtfilelegacy\"\n#         if not os.path.exists(os.path.join(data_folder, image_file)):\n#             image_file = f\"{text_id}.jpegfile\"\n\n#         text_path = os.path.join(data_folder, text_file)\n#         image_path = os.path.join(data_folder, image_file)\n\n#         if os.path.exists(text_path) and os.path.exists(image_path):\n#             aligned_texts.append(text_path)\n#             aligned_images.append(image_path)\n#             aligned_labels.append(label)\n\n#     print(f\"Aligned data: {len(aligned_texts)} texts, {len(aligned_images)} images, {len(aligned_labels)} labels.\")\n#     return aligned_texts, aligned_images, aligned_labels\n\n# # Load text and image data from the data folder\n# def load_text_and_images(data_folder, result_file):\n#     labels = load_labels(result_file)\n#     return align_data(data_folder, labels)\n\n# # Preprocessing\n# data_folder = \"/kaggle/input/mvsasingle/MVSA_Single/data\"\n# result_file = \"/kaggle/input/mvsasingle/MVSA_Single/labelResultAll.txt\"\n\n# texts, images, labels = load_text_and_images(data_folder, result_file)\n# print(f\"Total valid data points after filtering: {len(texts)}\")\n\n# # Check if data is available for train/test split\n# if len(texts) == 0:\n#     print(\"No valid data found after filtering. Please check your dataset.\")\n# else:\n#     # Train, validation, test split\n#     # train_texts, test_texts, train_images, test_images, train_labels, test_labels = train_test_split(\n#     #     texts, images, labels, test_size=0.30, random_state=42, stratify=labels\n#     # )\n\n#     # train_texts, val_texts, train_images, val_images, train_labels, val_labels = train_test_split(\n#     #     train_texts, train_images, train_labels, test_size=0.5, random_state=42, stratify=train_labels\n#     # )\n\n\n#     # First split: 70% for training, 30% for validation and testing\n#     train_texts, test_val_texts, train_images, test_val_images, train_labels, test_val_labels = train_test_split(\n#         texts, images, labels, test_size=0.30, random_state=42, stratify=labels\n#     )\n\n#     # Second split: 50% of the 30% split into validation and testing (15% each)\n#     val_texts, test_texts, val_images, test_images, val_labels, test_labels = train_test_split(\n#         test_val_texts, test_val_images, test_val_labels, test_size=0.5, random_state=42, stratify=test_val_labels\n#     )\n\n#     # Tokenizer and transforms\n#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n#     train_transform = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n#     val_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor()])\n\n#     # Datasets and dataloaders\n#     train_dataset = MultimodalDataset(train_texts, train_images, train_labels, tokenizer, train_transform)\n#     val_dataset = MultimodalDataset(val_texts, val_images, val_labels, tokenizer, val_transform)\n#     test_dataset = MultimodalDataset(test_texts, test_images, test_labels, tokenizer, val_transform)\n\n#     train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n#     val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n#     test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n#     # Model setup\n#     bert_model = BertModel.from_pretrained('bert-base-uncased')\n#     resnet_model = models.resnet50(pretrained=True)\n#     resnet_model.fc = nn.Identity()\n\n#     model = MultimodalSentimentModelPL(bert_model, resnet_model, num_classes=2)\n\n#     # Early stopping and model checkpointing\n#     early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=True, mode=\"min\")\n#     checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_loss\", dirpath=\"./checkpoints\", filename=\"best-checkpoint\", save_top_k=1, mode=\"min\"\n#     )\n\n#     # Trainer\n#     trainer = pl.Trainer(\n#         max_epochs=30,\n#         callbacks=[early_stopping, checkpoint_callback],\n#         logger=wandb_logger,\n#         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n#         log_every_n_steps=0\n#     )\n\n#     trainer.fit(model, train_loader, val_loader)\n#     trainer.test(model, test_loader)\n\n#     # Evaluate model on test set\n#     all_preds, all_labels = [], []\n#     for batch in test_loader:\n#         with torch.no_grad():\n#             input_ids = batch['text']\n#             attention_mask = batch['attention_mask']\n#             images = batch['image']\n#             labels = batch['label']\n\n#             outputs = model(input_ids=input_ids, attention_mask=attention_mask, image=images)\n#             preds = torch.argmax(outputs, dim=1)\n#             all_preds.extend(preds.cpu().numpy())\n#             all_labels.extend(labels.cpu().numpy())\n\n#     # Classification Report\n#     print(\"Classification Report:\")\n#     print(classification_report(all_labels, all_preds, target_names=['Negative', 'Positive']))\n\n#     precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n#     wandb.log({\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1})\n\n#     # Confusion Matrix\n#     cm = confusion_matrix(all_labels, all_preds)\n#     wandb.log({\"Confusion Matrix\": wandb.plot.confusion_matrix(\n#         probs=None,\n#         y_true=all_labels,\n#         preds=all_preds,\n#         class_names=['Negative', 'Positive']\n#     )})\n\n#     # Plot and display confusion matrix\n#     plt.figure(figsize=(8, 6))\n#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n#     plt.title(\"Confusion Matrix\")\n#     plt.xlabel(\"Predicted\")\n#     plt.ylabel(\"Actual\")\n#     plt.show()\n\n#     # Finish WandB run\n#     wandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:19:52.159291Z","iopub.execute_input":"2024-12-09T23:19:52.159582Z","iopub.status.idle":"2024-12-09T23:19:52.173731Z","shell.execute_reply.started":"2024-12-09T23:19:52.159555Z","shell.execute_reply":"2024-12-09T23:19:52.172771Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"!pip install torchsummary\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:19:52.175119Z","iopub.execute_input":"2024-12-09T23:19:52.175502Z","iopub.status.idle":"2024-12-09T23:20:01.843852Z","shell.execute_reply.started":"2024-12-09T23:19:52.175456Z","shell.execute_reply":"2024-12-09T23:20:01.842955Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchsummary in /opt/conda/lib/python3.10/site-packages (1.5.1)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import numpy as np\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import torchvision.transforms as transforms\n# import torchvision.models as models\n# from torch.utils.data import Dataset, DataLoader\n# from transformers import BertTokenizer, BertModel\n# import pytorch_lightning as pl\n# from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n# from pytorch_lightning.loggers import WandbLogger\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n# from PIL import Image\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import wandb\n# from nltk.corpus import stopwords\n# from nltk.tokenize import word_tokenize\n# from nltk.stem import WordNetLemmatizer\n# import re\n# import nltk\n# import time\n\n# # Set WandB environment variable to avoid socket issues\n# os.environ[\"WANDB_SOCKET_TIMEOUT\"] = \"300\"\n\n# # Initialize WandB with a unique run name and log hyperparameters\n# wandb.init(\n#     project=\"multimodal_sentiment_analysis\",\n#     name=f\"fusion_model_{int(time.time())}\",\n#     config={\n#         \"batch_size\": 32,\n#         \"learning_rate\": 2e-5,\n#         \"epochs\": 30,\n#         \"num_classes\": 2,\n#         \"dropout_rate\": 0.6,\n#         \"optimizer\": \"Adam\",\n#         \"scheduler\": \"CosineAnnealingLR\"\n#     }\n# )\n\n# # Initialize the WandbLogger for PyTorch Lightning\n# wandb_logger = WandbLogger()\n\n# # Device configuration\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # NLTK setup\n# nltk.download('stopwords')\n# nltk.download('punkt')\n# nltk.download('wordnet')\n\n# stop_words = set(stopwords.words(\"english\"))\n# lemmatizer = WordNetLemmatizer()\n\n# # Text preprocessing function\n# def preprocess_text(text):\n#     text = text.lower()\n#     text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n#     text = re.sub(r'[^\\w\\s]', '', text)\n#     text = re.sub(r'\\d+', '', text)\n#     tokens = word_tokenize(text)\n#     tokens = [word for word in tokens if word not in stop_words]\n#     lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n#     return \" \".join(lemmas)\n\n# # Dataset class\n# class MultimodalDataset(Dataset):\n#     def __init__(self, texts, images, labels, tokenizer, transform):\n#         self.texts = texts\n#         self.images = images\n#         self.labels = labels\n#         self.tokenizer = tokenizer\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.texts)\n\n#     def __getitem__(self, idx):\n#         text = self.texts[idx]\n#         image_path = self.images[idx]\n#         label = self.labels[idx]\n\n#         try:\n#             image = Image.open(image_path).convert('RGB')\n#         except FileNotFoundError:\n#             image = Image.new('RGB', (224, 224))\n\n#         processed_text = preprocess_text(text)\n#         encoded_text = self.tokenizer(processed_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n#         image = self.transform(image)\n\n#         return {\n#             'text': encoded_text['input_ids'].squeeze(),\n#             'attention_mask': encoded_text['attention_mask'].squeeze(),\n#             'image': image,\n#             'label': torch.tensor(label, dtype=torch.long)\n#         }\n\n# # PyTorch Lightning Model\n# class MultimodalSentimentModelPL(pl.LightningModule):\n#     def __init__(self, bert_model, resnet_model, num_classes):\n#         super(MultimodalSentimentModelPL, self).__init__()\n#         self.text_model = bert_model\n#         self.image_model = resnet_model\n#         self.fc1 = nn.Linear(768 + 2048, 512)\n#         self.fc2 = nn.Linear(512, num_classes)\n#         self.dropout = nn.Dropout(0.6)\n#         self.criterion = nn.CrossEntropyLoss()\n\n#     def forward(self, input_ids, attention_mask, image):\n#         input_ids = input_ids.to(self.device)\n#         attention_mask = attention_mask.to(self.device)\n#         image = image.to(self.device)\n        \n#         text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n#         image_output = self.image_model(image)\n#         combined = torch.cat((text_output, image_output), dim=1)\n#         x = self.fc1(combined)\n#         x = torch.relu(x)\n#         x = self.dropout(x)\n#         x = self.fc2(x)\n#         return x\n\n#     def training_step(self, batch, batch_idx):\n#         input_ids = batch['text']\n#         attention_mask = batch['attention_mask']\n#         images = batch['image']\n#         labels = batch['label']\n#         outputs = self(input_ids, attention_mask, images)\n#         loss = self.criterion(outputs, labels)\n#         preds = torch.argmax(outputs, dim=1)\n#         accuracy = (preds == labels).float().mean()\n#         self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n#         self.log(\"train_accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n#         return loss\n\n#     def validation_step(self, batch, batch_idx):\n#         input_ids = batch['text']\n#         attention_mask = batch['attention_mask']\n#         images = batch['image']\n#         labels = batch['label']\n#         outputs = self(input_ids, attention_mask, images)\n#         loss = self.criterion(outputs, labels)\n#         preds = torch.argmax(outputs, dim=1)\n#         accuracy = (preds == labels).float().mean()\n#         self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n#         self.log(\"val_accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n#         return loss\n\n#     def test_step(self, batch, batch_idx):\n#         input_ids = batch['text']\n#         attention_mask = batch['attention_mask']\n#         images = batch['image']\n#         labels = batch['label']\n#         outputs = self(input_ids, attention_mask, images)\n#         loss = self.criterion(outputs, labels)\n#         preds = torch.argmax(outputs, dim=1)\n#         accuracy = (preds == labels).float().mean()\n#         self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n#         self.log(\"test_accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n#         return loss\n\n#     def configure_optimizers(self):\n#         optimizer = optim.Adam(self.parameters(), lr=2e-5, weight_decay=1e-4)\n#         scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n#         return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n# # Filter dataset to exclude neutral labels\n# def filter_neutral_classes(texts, images, labels):\n#     filtered_texts, filtered_images, filtered_labels = [], [], []\n#     for text, image, label in zip(texts, images, labels):\n#         if label != 1:  # Exclude neutral class\n#             filtered_texts.append(text)\n#             filtered_images.append(image)\n#             filtered_labels.append(label)\n#     return filtered_texts, filtered_images, filtered_labels\n\n# # Load labels from the result file\n# def load_labels(result_file):\n#     labels = {}\n#     with open(result_file, 'r') as file:\n#         next(file)  # Skip header if present\n#         for line in file:\n#             parts = line.strip().split('\\t')\n#             text_id = int(parts[0])\n#             label_string = parts[1].strip()\n\n#             # Split label string into text_label and image_label\n#             text_label, image_label = label_string.split(',')\n\n#             # Only include rows where text_label is positive or negative\n#             if text_label in ['positive', 'negative']:\n#                 labels[text_id] = 1 if text_label == \"positive\" else 0\n\n#     print(f\"Loaded {len(labels)} valid labels.\")\n#     return labels\n\n# # Align text, images, and labels based on index\n# def align_data(data_folder, labels):\n#     aligned_texts, aligned_images, aligned_labels = [], [], []\n#     for text_id, label in labels.items():\n#         text_file = f\"{text_id}.txt\"\n#         image_file = f\"{text_id}.jpg\"\n\n#         if not os.path.exists(os.path.join(data_folder, text_file)):\n#             text_file = f\"{text_id}.txtfilelegacy\"\n#         if not os.path.exists(os.path.join(data_folder, image_file)):\n#             image_file = f\"{text_id}.jpegfile\"\n\n#         text_path = os.path.join(data_folder, text_file)\n#         image_path = os.path.join(data_folder, image_file)\n#         if os.path.exists(text_path) and os.path.exists(image_path):\n#             aligned_texts.append(text_path)\n#             aligned_images.append(image_path)\n#             aligned_labels.append(label)\n\n#     print(f\"Aligned data: {len(aligned_texts)} texts, {len(aligned_images)} images, {len(aligned_labels)} labels.\")\n#     return aligned_texts, aligned_images, aligned_labels\n\n# # Load text and image data from the data folder\n# def load_text_and_images(data_folder, result_file):\n#     labels = load_labels(result_file)\n#     return align_data(data_folder, labels)\n\n# # Preprocessing\n# data_folder = \"/kaggle/input/mvsasingle/MVSA_Single/data\"\n# result_file = \"/kaggle/input/mvsasingle/MVSA_Single/labelResultAll.txt\"\n\n# texts, images, labels = load_text_and_images(data_folder, result_file)\n# print(f\"Total valid data points after filtering neutral labels: {len(texts)}\")\n\n# # Check if data is available for train/test split\n# if len(texts) == 0:\n#     print(\"No valid data found after filtering neutral labels. Please check your dataset.\")\n# else:\n#     # Train, validation, test split\n#     train_texts, test_texts, train_images, test_images, train_labels, test_labels = train_test_split(\n#         texts, images, labels, test_size=0.15, random_state=42, stratify=labels\n#     )\n\n#     train_texts, val_texts, train_images, val_images, train_labels, val_labels = train_test_split(\n#         train_texts, train_images, train_labels, test_size=0.15/(1-0.15), random_state=42, stratify=train_labels\n#     )\n\n#     # Tokenizer and transforms\n#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n#     train_transform = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n#     val_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor()])\n\n#     # Datasets and dataloaders\n#     train_dataset = MultimodalDataset(train_texts, train_images, train_labels, tokenizer, train_transform)\n#     val_dataset = MultimodalDataset(val_texts, val_images, val_labels, tokenizer, val_transform)\n#     test_dataset = MultimodalDataset(test_texts, test_images, test_labels, tokenizer, val_transform)\n\n#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n#     val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n#     test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n#     # Model setup\n#     bert_model = BertModel.from_pretrained('bert-base-uncased')\n#     resnet_model = models.resnet50(pretrained=True)\n#     resnet_model.fc = nn.Identity()\n\n#     model = MultimodalSentimentModelPL(bert_model, resnet_model, num_classes=2)\n\n#     # Early stopping and model checkpointing\n#     early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=True, mode=\"min\")\n#     checkpoint_callback = ModelCheckpoint(\n#         monitor=\"val_loss\", dirpath=\"./checkpoints\", filename=\"best-checkpoint\", save_top_k=1, mode=\"min\"\n#     )\n\n#     # Trainer\n#     trainer = pl.Trainer(\n#         max_epochs=30,\n#         callbacks=[early_stopping, checkpoint_callback],\n#         logger=wandb_logger,\n#         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n#         log_every_n_steps=50\n#     )\n\n#     trainer.fit(model, train_loader, val_loader)\n#     trainer.test(model, test_loader)\n\n#     # Evaluate model on test set\n#     all_preds, all_labels = [], []\n#     for batch in test_loader:\n#         with torch.no_grad():\n#             input_ids = batch['text']\n#             attention_mask = batch['attention_mask']\n#             images = batch['image']\n#             labels = batch['label']\n\n#             outputs = model(input_ids=input_ids, attention_mask=attention_mask, image=images)\n#             preds = torch.argmax(outputs, dim=1)\n#             all_preds.extend(preds.cpu().numpy())\n#             all_labels.extend(labels.cpu().numpy())\n\n#     # Classification Report\n#     print(\"Classification Report:\")\n#     print(classification_report(all_labels, all_preds, target_names=['Negative', 'Positive']))\n\n#     precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n#     wandb.log({\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1})\n\n#     # Confusion Matrix\n#     cm = confusion_matrix(all_labels, all_preds)\n#     wandb.log({\"Confusion Matrix\": wandb.plot.confusion_matrix(\n#         probs=None,\n#         y_true=all_labels,\n#         preds=all_preds,\n#         class_names=['Negative', 'Positive']\n#     )})\n\n#     # Plot and display confusion matrix\n#     plt.figure(figsize=(8, 6))\n#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n#     plt.title(\"Confusion Matrix\")\n#     plt.xlabel(\"Predicted\")\n#     plt.ylabel(\"Actual\")\n#     plt.show()\n\n#     # Finish WandB run\n#     wandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:01.846043Z","iopub.execute_input":"2024-12-09T23:20:01.846371Z","iopub.status.idle":"2024-12-09T23:20:01.859579Z","shell.execute_reply.started":"2024-12-09T23:20:01.846341Z","shell.execute_reply":"2024-12-09T23:20:01.858650Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# def count_matching_pairs_with_labels(file_path):\n#     \"\"\"Count pairs where text and image labels match after excluding neutral labels, and print the matching IDs and labels.\"\"\"\n#     total_count = 0  # Count of valid pairs\n#     matching_pairs = []  # List to store the IDs and labels of matching pairs\n\n#     with open(file_path, 'r') as file:\n#         for line in file:\n#             # Parse the line: Assume format is \"<ID>\\t<text_label>,<image_label>\"\n#             parts = line.strip().split('\\t')\n#             if len(parts) != 2:\n#                 continue  # Skip malformed lines\n\n#             # Extract the labels\n#             id_number, labels = parts\n#             text_label, image_label = labels.split(',')\n\n#             # Exclude pairs with \"neutral\" in either text or image label\n#             if \"neutral\" in text_label or \"neutral\" in image_label:\n#                 continue\n\n#             # Count pairs where the labels match and save their IDs and labels\n#             if text_label.strip() == image_label.strip():\n#                 total_count += 1\n#                 matching_pairs.append((id_number, f\"{text_label.strip()}-{image_label.strip()}\"))\n\n#     # Print results\n#     print(f\"Total valid aligned pairs: {total_count}\")\n#     print(\"Matching IDs and Labels:\")\n#     for id_number, label_pair in matching_pairs:\n#         print(f\"ID: {id_number}, Label: {label_pair}\")\n\n#     return total_count, matching_pairs\n\n# # Example usage\n# file_path = \"/kaggle/input/mvsasingle/MVSA_Single/labelResultAll.txt\"\n# total_count, matching_pairs = count_matching_pairs_with_labels(file_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:01.862608Z","iopub.execute_input":"2024-12-09T23:20:01.862863Z","iopub.status.idle":"2024-12-09T23:20:01.875139Z","shell.execute_reply.started":"2024-12-09T23:20:01.862839Z","shell.execute_reply":"2024-12-09T23:20:01.874178Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# def print_neutral_pairs(file_path):\n#     \"\"\"Print pairs where both text and image labels are 'neutral'.\"\"\"\n#     neutral_pairs = []  # List to store IDs with both neutral labels\n\n#     with open(file_path, 'r') as file:\n#         for line in file:\n#             # Parse the line: Assume format is \"<ID>\\t<text_label>,<image_label>\"\n#             parts = line.strip().split('\\t')\n#             if len(parts) != 2:\n#                 continue  # Skip malformed lines\n\n#             # Extract the labels\n#             id_number, labels = parts\n#             text_label, image_label = labels.split(',')\n\n#             # Check if both labels are 'neutral'\n#             if text_label.strip() == \"neutral\" and image_label.strip() == \"neutral\":\n#                 neutral_pairs.append(id_number)\n\n#     # Print the results\n#     print(f\"Pairs with both 'neutral' labels ({len(neutral_pairs)} total):\")\n#     for pair_id in neutral_pairs:\n#         print(f\"ID: {pair_id}\")\n\n# # Example usage\n# file_path = \"/kaggle/input/mvsasingle/MVSA_Single/labelResultAll.txt\"\n# print_neutral_pairs(file_path)\n\n# #Pairs with both 'neutral' labels (470 total):","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:01.876445Z","iopub.execute_input":"2024-12-09T23:20:01.877089Z","iopub.status.idle":"2024-12-09T23:20:01.890267Z","shell.execute_reply.started":"2024-12-09T23:20:01.877049Z","shell.execute_reply":"2024-12-09T23:20:01.889463Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# import os\n\n# def align_data_with_label_matching(data_folder, labels):\n#     aligned_texts, aligned_images, aligned_labels = [], [], []\n#     total_count = 0  # To keep track of the valid data count\n\n#     for text_id, label in labels.items():\n#         # Construct the file paths for the text and image\n#         text_file = f\"{text_id}.txt\"\n#         image_file = f\"{text_id}.jpg\"\n\n#         # Fallback filenames in case the expected ones don't exist\n#         if not os.path.exists(os.path.join(data_folder, text_file)):\n#             text_file = f\"{text_id}.txtfilelegacy\"\n#         if not os.path.exists(os.path.join(data_folder, image_file)):\n#             image_file = f\"{text_id}.jpegfile\"\n\n#         text_path = os.path.join(data_folder, text_file)\n#         image_path = os.path.join(data_folder, image_file)\n\n#         # Check if both text and image exist\n#         if os.path.exists(text_path) and os.path.exists(image_path):\n#             # Ensure text and image have the same label before appending\n#             text_label = label  # Assuming label is text label (positive/negative)\n#             image_label = labels.get(text_id)  # Assuming same label is used for image\n            \n#             # Debugging: Print the file paths and labels\n#             print(f\"Processing {text_id}: Text label = {text_label}, Image label = {image_label}\")\n            \n#             # Only add the pair if both labels match\n#             if text_label == image_label:  # Ensure text and image have the same label\n#                 aligned_texts.append(text_path)\n#                 aligned_images.append(image_path)\n#                 aligned_labels.append(text_label)\n#                 total_count += 1  # Increment count for each valid pair\n#             else:\n#                 print(f\"Skipping: {text_id} - {text_file} and {image_file} (labels mismatch)\")\n#         else:\n#             print(f\"Skipping: {text_id} - Missing files (text or image not found)\")\n\n#     # Debug: Check the total count after processing all pairs\n#     print(f\"Total count of valid aligned pairs: {total_count}\")\n    \n#     return aligned_texts, aligned_images, aligned_labels, total_count\n\n# # Example usage\n# data_folder = \"/kaggle/input/mvsasingle/MVSA_Single/data\"\n# labels = load_labels(\"/kaggle/input/mvsasingle/MVSA_Single/labelResultAll.txt\")\n\n# aligned_texts, aligned_images, aligned_labels, total_count = align_data_with_label_matching(data_folder, labels)\n# print(f\"Total valid aligned pairs: {total_count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:01.891223Z","iopub.execute_input":"2024-12-09T23:20:01.891477Z","iopub.status.idle":"2024-12-09T23:20:01.903565Z","shell.execute_reply.started":"2024-12-09T23:20:01.891454Z","shell.execute_reply":"2024-12-09T23:20:01.902448Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# import os\n\n# # Define the path to the label file\n# result_file = \"/kaggle/input/mvsasingle/MVSA_Single/labelResultAll.txt\"\n\n# def load_labels(result_file):\n#     labels = {}\n#     if not os.path.exists(result_file):\n#         print(f\"Error: File not found at {result_file}\")\n#         return {}\n\n#     with open(result_file, 'r') as file:\n#         header = next(file)  # Skip the header\n#         print(f\"Header: {header.strip()}\")\n\n#         line_count = 0  # Count total lines processed\n#         for line in file:\n#             line_count += 1\n#             parts = line.strip().split('\\t')\n            \n#             # Ensure the line has two parts: ID and labels\n#             if len(parts) < 2:\n#                 print(f\"Skipping invalid line {line_count}: {line.strip()}\")\n#                 continue\n\n#             text_id = int(parts[0])\n#             label_string = parts[1].strip()\n\n#             # Split into text_label and image_label\n#             if ',' not in label_string:\n#                 print(f\"Skipping invalid label format at line {line_count}: {label_string}\")\n#                 continue\n\n#             text_label, image_label = label_string.split(',')\n\n#             # Only include rows where text_label is positive or negative\n#             if text_label in ['positive', 'negative']:\n#                 labels[text_id] = 1 if text_label == \"positive\" else 0\n#             else:\n#                 print(f\"Skipping line {line_count}: Text label is neutral.\")\n\n#     print(f\"Processed {line_count} lines.\")\n#     print(f\"Loaded {len(labels)} valid text labels.\")\n#     print(f\"First 10 labels: {list(labels.items())[:10]}\")\n#     return labels\n\n# # Execute the function and print the results\n# labels = load_labels(result_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:01.904667Z","iopub.execute_input":"2024-12-09T23:20:01.904997Z","iopub.status.idle":"2024-12-09T23:20:01.918485Z","shell.execute_reply.started":"2024-12-09T23:20:01.904968Z","shell.execute_reply":"2024-12-09T23:20:01.917780Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# import os\n\n# data_folder = \"/kaggle/input/mvsasingle/MVSA_Single/data\"\n# result_file = \"/kaggle/input/mvsasingle/MVSA_Single/labelResultAll.txt\"\n\n# def load_labels(result_file):\n#     labels = {}\n#     with open(result_file, 'r') as file:\n#         header = next(file)\n#         for line in file:\n#             parts = line.strip().split('\\t')\n#             text_id = int(parts[0])\n#             label_string = parts[1].strip()\n#             text_label, image_label = label_string.split(',')\n\n#             if text_label in ['positive', 'negative']:\n#                 labels[text_id] = 1 if text_label == \"positive\" else 0\n\n#     print(f\"Loaded {len(labels)} valid labels.\")\n#     return labels\n\n# def align_data(data_folder, labels):\n#     aligned_texts, aligned_images, aligned_labels = [], [], []\n#     missing_files = []\n\n#     for text_id, label in labels.items():\n#         # Construct expected filenames\n#         text_file = f\"{text_id}.txt\"\n#         image_file = f\"{text_id}.jpg\"\n\n#         # Handle alternative extensions\n#         if not os.path.exists(os.path.join(data_folder, text_file)):\n#             text_file = f\"{text_id}.txtfilelegacy\"\n#         if not os.path.exists(os.path.join(data_folder, image_file)):\n#             image_file = f\"{text_id}.jpegfile\"\n\n#         # Check for missing files\n#         text_path = os.path.join(data_folder, text_file)\n#         image_path = os.path.join(data_folder, image_file)\n#         if os.path.exists(text_path) and os.path.exists(image_path):\n#             aligned_texts.append(text_file)\n#             aligned_images.append(image_file)\n#             aligned_labels.append(label)\n#         else:\n#             missing_files.append(text_id)\n\n#     if missing_files:\n#         print(f\"Missing files for text IDs: {missing_files[:10]} (showing up to 10)\")\n#         print(f\"Total missing files: {len(missing_files)}\")\n\n#     print(f\"Aligned data: {len(aligned_texts)} texts, {len(aligned_images)} images, {len(aligned_labels)} labels.\")\n#     return aligned_texts, aligned_images, aligned_labels\n\n# def load_text_and_images(data_folder, result_file):\n#     labels = load_labels(result_file)\n#     return align_data(data_folder, labels)\n\n# # Debugging the alignment\n# texts, images, labels = load_text_and_images(data_folder, result_file)\n# print(f\"Total valid data points after filtering neutral labels: {len(texts)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:01.919707Z","iopub.execute_input":"2024-12-09T23:20:01.920042Z","iopub.status.idle":"2024-12-09T23:20:01.934655Z","shell.execute_reply.started":"2024-12-09T23:20:01.920016Z","shell.execute_reply":"2024-12-09T23:20:01.933752Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import numpy as np\n# import torch\n# import torchvision.transforms as transforms\n# from torch.utils.data import Dataset, DataLoader\n# from transformers import BertTokenizer, BertModel\n# from sklearn.model_selection import train_test_split\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.optim.lr_scheduler import ReduceLROnPlateau\n# import torchvision.models as models\n# from sklearn.metrics import confusion_matrix, classification_report\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from PIL import Image\n# from nltk.corpus import stopwords\n# from nltk.tokenize import word_tokenize\n# from nltk.stem import WordNetLemmatizer\n# import re\n# import nltk\n\n# # Download NLTK resources\n# nltk.download('stopwords')\n# nltk.download('punkt')\n# nltk.download('wordnet')\n\n# # Device configuration\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:01.935775Z","iopub.execute_input":"2024-12-09T23:20:01.936118Z","iopub.status.idle":"2024-12-09T23:20:01.949366Z","shell.execute_reply.started":"2024-12-09T23:20:01.936090Z","shell.execute_reply":"2024-12-09T23:20:01.948450Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# stop_words = set(stopwords.words(\"english\"))\n# lemmatizer = WordNetLemmatizer()\n\n# def preprocess_text(text):\n#     text = text.lower()\n#     text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n#     text = re.sub(r'[^\\w\\s]', '', text)\n#     text = re.sub(r'\\d+', '', text)\n#     tokens = word_tokenize(text)\n#     tokens = [word for word in tokens if word not in stop_words]\n#     lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n#     return \" \".join(lemmas)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:01.950302Z","iopub.execute_input":"2024-12-09T23:20:01.950537Z","iopub.status.idle":"2024-12-09T23:20:01.962177Z","shell.execute_reply.started":"2024-12-09T23:20:01.950515Z","shell.execute_reply":"2024-12-09T23:20:01.961433Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# def load_text_data(data_folder):\n#     texts = []\n#     filenames = sorted(os.listdir(data_folder), key=lambda x: int(x[:-4]) if x[:-4].isdigit() else x)\n#     for filename in filenames:\n#         if filename.endswith(\".txt\"):\n#             with open(os.path.join(data_folder, filename), 'r', encoding='latin-1') as file:\n#                 text = file.read().strip()\n#                 texts.append(text)\n#     return texts, filenames\n\n# def load_labels(result_file):\n#     labels = {}\n#     with open(result_file, 'r') as file:\n#         next(file)  # Skip header\n#         for line in file:\n#             parts = line.strip().split('\\t')\n#             text_id = int(parts[0])\n#             text_label, image_label = parts[1].split(',')\n#             labels[text_id] = (text_label.strip(), image_label.strip())\n#     return labels\n\n# def filter_existing_files(texts, filenames, labels, data_folder):\n#     existing_texts, existing_images, existing_labels = [], [], []\n#     for i, text in enumerate(texts):\n#         image_file = os.path.join(data_folder, f\"{i+1}.jpg\")\n#         if os.path.exists(image_file) and (i+1) in labels:\n#             existing_texts.append(text)\n#             existing_images.append(image_file)\n#             existing_labels.append(labels[i+1])\n#     return existing_texts, existing_images, existing_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:01.963186Z","iopub.execute_input":"2024-12-09T23:20:01.963473Z","iopub.status.idle":"2024-12-09T23:20:01.973058Z","shell.execute_reply.started":"2024-12-09T23:20:01.963450Z","shell.execute_reply":"2024-12-09T23:20:01.972089Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# class MultimodalDataset(Dataset):\n#     def __init__(self, texts, images, labels, tokenizer, transform):\n#         self.texts = texts\n#         self.images = images\n#         self.labels = labels\n#         self.tokenizer = tokenizer\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.texts)\n\n#     def __getitem__(self, idx):\n#         text = self.texts[idx]\n#         image_path = self.images[idx]\n#         text_label, image_label = self.labels[idx]\n\n#         try:\n#             image = Image.open(image_path).convert('RGB')\n#         except FileNotFoundError:\n#             print(f\"File not found: {image_path}\")\n#             image = Image.new('RGB', (224, 224))  # Blank image\n\n#         processed_text = preprocess_text(text)\n#         encoded_text = self.tokenizer(processed_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n#         image = self.transform(image)\n#         label = self.sentiment_to_label[multimodal_label(text_label, image_label)]\n\n#         return {\n#             'text': encoded_text['input_ids'].squeeze(),\n#             'attention_mask': encoded_text['attention_mask'].squeeze(),\n#             'image': image,\n#             'label': torch.tensor(label, dtype=torch.long)\n#         }\n\n#     sentiment_to_label = {'negative': 0, 'neutral': 1, 'positive': 2}\n\n# def multimodal_label(text_label, image_label):\n#     if text_label == image_label:\n#         return text_label\n#     elif text_label == 'neutral':\n#         return image_label\n#     elif image_label == 'neutral':\n#         return text_label\n#     else:\n#         return image_label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:01.973896Z","iopub.execute_input":"2024-12-09T23:20:01.974222Z","iopub.status.idle":"2024-12-09T23:20:01.990414Z","shell.execute_reply.started":"2024-12-09T23:20:01.974195Z","shell.execute_reply":"2024-12-09T23:20:01.989496Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# data_folder = \"/kaggle/input/mvsasingle/MVSA_Single/data/\"\n# result_file = '/kaggle/input/mvsasingle/MVSA_Single/labelResultAll.txt'\n\n# texts, filenames = load_text_data(data_folder)\n# labels = load_labels(result_file)\n# texts, image_paths, labels = filter_existing_files(texts, filenames, labels, data_folder)\n\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# transform = transforms.Compose([\n#     transforms.Resize(256),\n#     transforms.CenterCrop(224),\n#     transforms.ToTensor(),\n#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n# ])\n\n# train_texts, temp_texts, train_images, temp_images, train_labels, temp_labels = train_test_split(\n#     texts, image_paths, labels, test_size=0.425, random_state=42, stratify=labels\n# )\n# val_texts, test_texts, val_images, test_images, val_labels, test_labels = train_test_split(\n#     temp_texts, temp_images, temp_labels, test_size=0.3529, random_state=42, stratify=temp_labels\n# )\n\n# train_dataset = MultimodalDataset(train_texts, train_images, train_labels, tokenizer, transform)\n# val_dataset = MultimodalDataset(val_texts, val_images, val_labels, tokenizer, transform)\n# test_dataset = MultimodalDataset(test_texts, test_images, test_labels, tokenizer, transform)\n\n# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:01.991505Z","iopub.execute_input":"2024-12-09T23:20:01.991793Z","iopub.status.idle":"2024-12-09T23:20:02.006680Z","shell.execute_reply.started":"2024-12-09T23:20:01.991761Z","shell.execute_reply":"2024-12-09T23:20:02.005831Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# class MultimodalSentimentModel(nn.Module):\n#     def __init__(self, bert_model, resnet_model, num_classes):\n#         super(MultimodalSentimentModel, self).__init__()\n#         self.text_model = bert_model\n#         self.image_model = resnet_model\n#         self.fc1 = nn.Linear(768 + 2048, 512)\n#         self.fc2 = nn.Linear(512, num_classes)\n#         self.dropout = nn.Dropout(0.5)\n\n#     def forward(self, input_ids, attention_mask, image):\n#         text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n#         image_output = self.image_model(image)\n#         combined = torch.cat((text_output, image_output), dim=1)\n#         x = self.fc1(combined)\n#         x = self.dropout(x)\n#         x = self.fc2(x)\n#         return x\n\n# bert_model = BertModel.from_pretrained('bert-base-uncased')\n# resnet_model = models.resnet50(pretrained=True)\n# resnet_model.fc = nn.Identity()\n# model = MultimodalSentimentModel(bert_model, resnet_model, num_classes=3).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:02.007606Z","iopub.execute_input":"2024-12-09T23:20:02.007882Z","iopub.status.idle":"2024-12-09T23:20:02.023470Z","shell.execute_reply.started":"2024-12-09T23:20:02.007854Z","shell.execute_reply":"2024-12-09T23:20:02.022636Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# # Initialize loss function, optimizer, and scheduler\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=2e-5)\n# scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n\n# # Lists to store training and validation metrics\n# train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n# best_val_loss = float('inf')\n# patience_counter = 0\n\n# # Training Loop\n# for epoch in range(30):\n#     # Training phase\n#     model.train()\n#     train_loss, train_correct, train_total = 0, 0, 0\n\n#     for batch in train_loader:\n#         input_ids = batch['text'].to(device)\n#         attention_mask = batch['attention_mask'].to(device)\n#         images = batch['image'].to(device)\n#         labels = batch['label'].to(device)\n\n#         optimizer.zero_grad()\n#         outputs = model(input_ids, attention_mask, images)\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n\n#         train_loss += loss.item()\n#         train_total += labels.size(0)\n#         train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n\n#     # Append training metrics\n#     train_losses.append(train_loss / len(train_loader))\n#     train_accuracies.append(train_correct / train_total)\n\n#     # Validation phase\n#     model.eval()\n#     val_loss, val_correct, val_total = 0, 0, 0\n\n#     with torch.no_grad():\n#         for batch in val_loader:\n#             input_ids = batch['text'].to(device)\n#             attention_mask = batch['attention_mask'].to(device)\n#             images = batch['image'].to(device)\n#             labels = batch['label'].to(device)\n\n#             outputs = model(input_ids, attention_mask, images)\n#             loss = criterion(outputs, labels)\n\n#             val_loss += loss.item()\n#             val_total += labels.size(0)\n#             val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n\n#     # Append validation metrics\n#     val_losses.append(val_loss / len(val_loader))\n#     val_accuracies.append(val_correct / val_total)\n\n#     # Print epoch metrics\n#     print(f'Epoch {epoch + 1}/30:')\n#     print(f'Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}')\n#     print(f'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}')\n\n#     # Scheduler step\n#     scheduler.step(val_losses[-1])\n\n#     # Early stopping\n#     if val_losses[-1] < best_val_loss:\n#         best_val_loss = val_losses[-1]\n#         patience_counter = 0\n#         torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n#     else:\n#         patience_counter += 1\n#         if patience_counter >= 10:  # Early stopping patience\n#             print(\"Early stopping\")\n#             break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:02.024898Z","iopub.execute_input":"2024-12-09T23:20:02.025281Z","iopub.status.idle":"2024-12-09T23:20:02.037770Z","shell.execute_reply.started":"2024-12-09T23:20:02.025233Z","shell.execute_reply":"2024-12-09T23:20:02.036757Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# # Load the best model\n# model.load_state_dict(torch.load('best_model.pth'))\n# model.eval()\n\n# test_loss, test_correct, test_total = 0, 0, 0\n# all_labels, all_preds = [], []\n\n# with torch.no_grad():\n#     for batch in test_loader:\n#         input_ids = batch['text'].to(device)\n#         attention_mask = batch['attention_mask'].to(device)\n#         images = batch['image'].to(device)\n#         labels = batch['label'].to(device)\n\n#         outputs = model(input_ids, attention_mask, images)\n#         loss = criterion(outputs, labels)\n\n#         test_loss += loss.item()\n#         test_total += labels.size(0)\n#         test_correct += (outputs.argmax(dim=1) == labels).sum().item()\n\n#         all_labels.extend(labels.cpu().numpy())\n#         all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n\n# test_loss /= len(test_loader)\n# test_accuracy = test_correct / test_total\n\n# print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:02.038883Z","iopub.execute_input":"2024-12-09T23:20:02.039195Z","iopub.status.idle":"2024-12-09T23:20:02.051537Z","shell.execute_reply.started":"2024-12-09T23:20:02.039171Z","shell.execute_reply":"2024-12-09T23:20:02.050803Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# # Plot Training and Validation Accuracy\n# plt.figure(figsize=(10, 5))\n# plt.plot(train_accuracies, label='Train Accuracy')\n# plt.plot(val_accuracies, label='Validation Accuracy')\n# plt.title('Accuracy per Epoch')\n# plt.xlabel('Epoch')\n# plt.ylabel('Accuracy')\n# plt.legend()\n# plt.grid()\n# plt.show()\n\n# # Plot Training and Validation Loss\n# plt.figure(figsize=(10, 5))\n# plt.plot(train_losses, label='Train Loss')\n# plt.plot(val_losses, label='Validation Loss')\n# plt.title('Loss per Epoch')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.grid()\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:02.052519Z","iopub.execute_input":"2024-12-09T23:20:02.052775Z","iopub.status.idle":"2024-12-09T23:20:02.064314Z","shell.execute_reply.started":"2024-12-09T23:20:02.052751Z","shell.execute_reply":"2024-12-09T23:20:02.063317Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# # Confusion Matrix\n# cm = confusion_matrix(all_labels, all_preds)\n# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n# plt.title('Confusion Matrix')\n# plt.xlabel('Predicted')\n# plt.ylabel('Actual')\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:02.065554Z","iopub.execute_input":"2024-12-09T23:20:02.066180Z","iopub.status.idle":"2024-12-09T23:20:02.073285Z","shell.execute_reply.started":"2024-12-09T23:20:02.066154Z","shell.execute_reply":"2024-12-09T23:20:02.072562Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# Classification Report\n#print(classification_report(all_labels, all_preds, target_names=['Negative', 'Neutral', 'Positive']))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:20:02.074568Z","iopub.execute_input":"2024-12-09T23:20:02.074849Z","iopub.status.idle":"2024-12-09T23:20:02.082889Z","shell.execute_reply.started":"2024-12-09T23:20:02.074824Z","shell.execute_reply":"2024-12-09T23:20:02.082175Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}